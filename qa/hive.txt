Apache Hive is built on top of 
Apache Hadoop
hive Query execution via 
Apache Tez, Apache Spark, or MapReduce 
Procedural language with 
HPL-SQL
Sub-second query retrieval via 
Hive LLAP, Apache YARN and Apache Slider
Hive's SQL can also be extended with user code via 
user defined functions (UDFs), user defined aggregates (UDAFs), and user defined table functions (UDTFs).
There is not a single Hive format in which data must be stored
Hive comes with built in connectors for 
comma and tab-separated values (CSV/TSV) text files, Apache Parquet™, Apache ORC™, and other formats.
Users can extend Hive with connectors for other formats using 
Hive SerDe 
Hive is not designed for 
online transaction processing (OLTP) workloads
It is best used for 
traditional data warehousing tasks.
Components of Hive include 
HCatalog and WebHCat.
HCatalog is a 
table and storage management layer for Hadoop 
HCatalog enables  Pig and MapReduce etc 
to more easily read and write data on the grid.
WebHCat provides a service that you can use to 
run Hadoop MapReduce (or YARN), Pig, Hive jobs. 
WebHCat provides a HTTP (REST style) interface for 
performing Hive metadata operations