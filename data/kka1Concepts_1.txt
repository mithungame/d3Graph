#?#box#calculatedX@281#calculatedY@229#children@45,1712464776749,1712640915627#name@0:seed#id@seed#parent@#x@281#y@229
title
#?#box#calculatedX@369#calculatedY@156#children@66,1712330435713,1712330830657#name@0:ksqldb#id@45#parent@#x@369#y@156
ksqlDB is
the event streaming database

In Kafka, you store a collection of events in a 
topic
Each event can contain any 
raw bytes 

In ksqlDB, you store events in a 
stream
stream is a topic with a strongly 
defined schema
#?#box#calculatedX@534#calculatedY@66#children@1712330229481#d_visibility@hidden#name@0:declareStream#id@66#parent@#x@534#y@66

CREATE STREAM readings (
    sensor VARCHAR KEY,
    location VARCHAR,
    reading INT
) WITH (
    kafka_topic = 'readings',
    partitions = 3,
    value_format = 'json'
);
#?#box#calculatedX@658#calculatedY@70#children@#name@1:behindScenes#id@1712330229481#parent@#x@658#y@70
if topic does not exist 
create topic
save metadata in a local 
metadata store
data is partitioned on sensor
#?#box#calculatedX@533#calculatedY@109#children@1712330531996#d_visibility@hidden#name@1:insertrow#id@1712330435713#parent@#x@533#y@109
INSERT INTO readings (sensor, location, reading) VALUES ('sensor-1', 'wheel', 45);

In Kafka, you model an event as a --- and put it into a ---
record,topic
In ksqlDB, you model an event as a --- and put it into a ---. 
row,stream
A row is just a record with 
additional metadata.
#?#box#calculatedX@659#calculatedY@110#children@#name@1:behindScenes#id@1712330531996#parent@#x@659#y@110
a request with the payload is sent to a ksqlDB server
check schema
reject malformed records
use Kafka producer client to insert that record into the backing Kafka topic
persisted on directly on the broker
None of it lives in ksqlDB servers
#?#box#calculatedX@528#calculatedY@158#children@1712331421750#d_visibility@hidden#name@2:transformations#id@1712330830657#parent@#x@528#y@158
kafka way - use consumer to transform producer data
this is low level

ksqldb way - issue a persistent query to transform one stream into another using its SQL


-- process from the beginning of each stream
set 'auto.offset.reset' = 'earliest';
CREATE STREAM clean AS
SELECT sensor,
reading,
UCASE(location) AS location
FROM readings
EMIT CHANGES;
#?#box#calculatedX@668#calculatedY@157#children@#name@1:behindScenes#id@1712331421750#parent@#x@668#y@157
Persistent queries are little stream processing programs that run indefinitely
ksqlDB server compiles query  to a physical execution plan as a Kafka Streams topology
The topology runs as a daemon, reacting to new topic records as soon as they become available
If you run ksqlDB as a cluster, the topology scales horizontally 
all of the processing work happens on ksqlDB server; 
no processing work happens on the Kafka brokers

When a persistent query is created, it is assigned a generated name
As each row passes through the persistent query, the transformation logic is applied to create a new row
Persistent queries completely manage their own processing progression, even in the presence of faults. ksqlDB durably maintains the highest offset of each input partition.
#?#box#calculatedX@372#calculatedY@232#children@1712464808098#name@2:kafka#id@1712464776749#parent@#x@372#y@232
content
#?#box#calculatedX@445#calculatedY@226#children@#name@1:keyConcepts#id@1712464808098#parent@#x@445#y@226
EVENT
An event records the fact that something happened

PRODUCERS
Producers are those client applications that publish (write) events to Kafka

CONSUMERS
consumers are those that subscribe to (read and process) these events. 

In Kafka, producers and consumers are fully decoupled and agnostic of each other

Events are organized and durably stored in topics

TOPICS
Topics in Kafka are always --- and ---
multi-producer and multi-subscriber
a topic can 0,1,N producers that write events to it, as well as 0,1,N consumers that subscribe to these events

PARTITION
Topics are partitioned
Kafka guarantees that any consumer of a given topic-partition will always read that partitions events in exactly the same order as they were written

REPLICATION
Topics are replicated
#?#box#calculatedX@351#calculatedY@306#children@1712641154508#name@3:kafkaStreams#id@1712640915627#parent@#x@351#y@306
Kafka Streams is a 
client library for processing and analyzing data stored in Kafka.

Supports exactly-once processing semantics to guarantee that each record will be processed once and only once even when there is a failure on either Streams clients or Kafka brokers in the middle of processing.
#?#box#calculatedX@458#calculatedY@305#children@#name@1:keyConcepts#id@1712641154508#parent@#x@458#y@305
link
https://kafka.apache.org/37/documentation/streams/core-concepts

A stream is the most important abstraction provided by Kafka Streams
A stream is 
an ordered, replayable, and fault-tolerant sequence of immutable data records, where a data record is defined as a key-value pair.

TOPOLOGY
A DAG of logical abstraction made of Source processor, Stream processor(nodes), Sink Processor. Streams are the edges which represent logic flow
At runtime, the logical topology is instantiated and replicated inside the application for parallel processing (see Stream Partitions and Tasks for details).

Source Processor
does not have any upstream processors
produces an input stream to its topology from one or multiple Kafka topics by consuming records from these topics

A sink processor does not have 
down-stream processors. 
It sends any received records from its up-stream processors to a 
specified Kafka topic.

procssor topology can be manipulated using 
Kafka Streams DSL provides the most common data transformation operations such as map, filter, join and aggregations out of the box
the lower-level Processor API allows developers define and connect custom processors as well as to interact with state stores.
#?#box#calculatedX@546.4#calculatedY@288#children@#name@rootNode#id@root#parent@#x@546#y@288
root Node
