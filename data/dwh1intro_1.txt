#?#box#calculatedX@108#calculatedY@98#children@1728912850,1728912869,1728920987#name@seed#id@seed#parent@#x@86#y@78#tag@dwh
content
#?#box#calculatedX@108#calculatedY@98#children@1728913303#name@datalake#id@1728912869#parent@#x@86#y@78
A data lake is a ~*central location~* that holds a large amount of data in its ~*native, raw format~* with metadata tags and a unique identifier
Data lakes consolidate all data in a single location "~*as is~*" without a upfront ~*schema~*
Data in all stages of the refinement process can be stored. Raw data can be ingested and stored alongside structured, tabular data source

zones
~*Raw/Landing~*
    Raw - As is 
    Landing - raw data thats from a recognized source system
    Note: Both in raw and landing data must be in ~*original format~*
~*Raw/Conformance~*
    Data gets converted into the ~*delta lake/other format~* and lands in an input folder. 
    When data quality runs, pass records are copied to output folder. Records that fail land in an error folder.
~*Standardized - Ready for analytics~*
    This data layer is considered the ~*silver layer~* or ~*read data source~*. 
    Data within this layer has had ~*no transformations~* applied other than data quality, delta lake conversion, and data type alignment.
~*Curated~*
    Ready for ~*optimized analytics~*
    ~*Dimensional modelling~*
    

=Disadvantages
Reliability
Slow performance as data increases
Security

#?#box#calculatedX@108#calculatedY@98#children@1728920181,1728920199#name@lakeHouse#id@1728913303#parent@#x@86#y@78
https://www.starburst.io/blog/iceberg-vs-delta-lake/
lakeHouse is a datalake with rules on top of it 
~*Deltalake~* and ~*iceberg~* are examples of lakeHouse format

#?#box#calculatedX@108#calculatedY@98#children@#name@deltaLake#id@1728920181#parent@#x@86#y@78
Deltalake is an open source ~*table storage format~* originally developed by Databricks in 2017(open sourced in 2019) as their ~*data lakehouse table format~*.
Deltalake overcomes lakehouse limitations, it is an open format data management and governance layer that combines the best of both data lakes and data warehouses
DeltaLake seems closely associated with ~*spark~* and ~*databricks~*

Features of DeltaLake
Ability to update and delete records quickly 
ACID compliance 
Schema Evolution
Time Travel 

#?#box#calculatedX@108#calculatedY@98#children@#name@Iceberg#id@1728920199#parent@#x@86#y@78
Apache Iceberg was originally designed by Netflix as a replacement for ~*Hive data lakes~* in 2017. Iceberg ran on Trino clusters. 
It was engineered to handle ~*larger datasets~* that changed ~*too frequently~* for Hive. 
To do this, Iceberg takes the same object storage that Hive used, but collects ~*additional metadata~*. 
Icebergs ~*metadata management~* is the key to its architecture, and allows for data warehouse-like functionality using cloud object storage. 
Since its origins at Netflix, Iceberg is governed by the Apache foundation, and is used as an open source table format in many data lakehouse solutions. 

#?#box#calculatedX@108#calculatedY@98#children@1728920981#name@fileFormat#id@1728920987#parent@#x@86#y@78
#?#box#calculatedX@108#calculatedY@98#children@#name@parquet#id@1728920981#parent@#x@86#y@78

#?#box#calculatedX@108#calculatedY@98#children@1720961745,1720961599#name@datawarehouse#id@1728912850#parent@#x@86#y@78#tag@dwh
content
#?#box#calculatedX@108#calculatedY@98#children@1728824252,1720961704,1720961816,1720961623,1720961660,1720961854#name@architecture#id@1720961599#parent@#x@86#y@78
content
#?#box#calculatedX@108#calculatedY@98#children@#name@coreKimball#id@1728824252#parent@#x@86#y@78
        ETL System:         Presentation Area:      BI Applications:
        = Transform from    = Dimensional (star     = Ad hoc queries
        source-to-target    schema or OLAP          = Standard reports
        = Conform           cube)                   = Analytic apps
        dimensions          = Atomic and            = Data mining and
        = Normalization     summary data            models
SOURCE  optional            = Organized by
SYSTEM  = No user query     business process
        support             = Uses conformed
        Design Goals:       dimensions
        = Throughput        Design Goals:
        = Integrity and     = Ease-of-use
        consistency         = Query performance
                            ==================
                            Enterprise DW Bus
                            Architecture
#?#box#calculatedX@108#calculatedY@98#children@#name@IndependentDataMart#id@1720961704#parent@#x@86#y@78
Independent datamart architecture
Not a Kimball model 
example each BI department gets a data mart of its own
source -> Stage -> datamart 1 -> BI application 1 
       -> Stage -> datamart 2 -> BI application 2  
problem is if departmentA needs customer data they form their own dimension , same for departmentB
Strongly discouraged by Kimball  but it help rapid development since each team is on its own 
sometimes each team has its own staging layer as seen in diagram
Maintenance nightmare
#?#box#calculatedX@108#calculatedY@98#children@#name@hubSpokeCIF#id@1720961816#parent@#x@86#y@78
hub and spoke CIF is advocated by Inmon
CIF - corporate information factory 
Bill Inmon model
source -> EDW(must be normalized) -> Multiple Data Marts
          user queryable             Dimensional 
          3 NF                       Summarized  
          Atomic                     Departmental

Organizations who have adopted the CIF approach often access the EDW repository for detailed queries
presentation area is often departmental and summarized
Not suitable for large companies as EDW layer is a perfect world 
#?#box#calculatedX@108#calculatedY@98#children@#name@ods#id@1720961623#parent@#x@86#y@78
ODS - ~* operational data store ~*
Front end
processing at row level
no historic data 
no user query allowed
lives outside datawarehouse
#?#box#calculatedX@108#calculatedY@98#children@#name@Staging#id@1720961660#parent@#x@86#y@78
staging reads from ods and is a ~*cleansing~* layer 
can be in ~*3NF~*,called as ~*EDW~* in Inmon, 3NF is also called ~*ER~* model 
Kimball rejects all this calls it ~*staging ~*. if staging is in ~*3NF~* , its called ~*Normalized staging~*. It is offlimits to query 
#?#box#calculatedX@108#calculatedY@98#children@1720961792,1728822873#name@DimensionalModel#id@1720961745#parent@#x@86#y@78
content
#?#box#calculatedX@108#calculatedY@98#children@#name@dimension#id@1720961792#parent@#x@86#y@78
Dimensions must be ~*atomic~* which allows ~*adhoc query~*
Dimensions must be ~*wide~* and ~*descriptive~*
Dimensions must be ~*conformed~* that allows ~*single source of truth~*
#?#box#calculatedX@108#calculatedY@98#children@1729452141,1729452196,1729452611#name@fact#id@1728822873#parent@#x@86#y@78
The term fact represents a ~*business measure~*
The data on each row is at a specific level of detail, referred to as the ~*grain~*, such as one row per product sold on a sales transaction.
facts can be ~*additive, semi-additive, non-additive ~*
Semi-additive facts, such as ~*account balances~*, cannot be summed across the time dimension
Non-additive facts, such as ~*unit prices~*, can never be added
~*Textual facts~* are very rare and must be placed in dimension unless there is a very valid reason ( text is unique for every row in the fact table) 
It is important not to fill ~*no activity items~* because these overwhelm most fact tables.
all fact table grains fall into one of three categories: ~*transaction, periodic snapshot, and accumulating snapshot.~*
Facts are ~*narrow and long~*, have two or more foreign keys that connect to the dimension tables primary keys
The fact table generally has its own primary key composed of a ~*subset of the foreign keys~* called ~*composite key~*.

example
Retail Sales Fact
==================
Date Key (FK)
Product Key (FK)
Store Key (FK)
Promotion Key (FK)
Customer Key (FK)
Clerk Key (FK)
Transaction #
Sales Dollars
Sales Units
#?#box#calculatedX@108#calculatedY@98#children@#name@Transactionfact#id@1729452141#parent@#x@86#y@78
A row in a transaction fact table corresponds to a measurement event at a point in ~*space and time~*
Transaction fact tables may be dense or sparse because rows exist only if ~*measurements take place~*
#?#box#calculatedX@108#calculatedY@98#children@#name@periodicSnapshot#id@1729452196#parent@#x@86#y@78
A row in a periodic snapshot fact table summarizes many measurement events occurring over a ~*standard period~*, such as a ~*day~*, ~*week~*, ~*month~*. 
The grain is the ~*period~*, not the individual transaction.
Often contain many facts because any measurement event consistent with the fact table grain is permissible. 
They have ~*uniformly dense~* foreign keys, even if no activity takes place during the period, a row is typically inserted in the fact table containing a ~*zero or null~* for each fact.
#?#box#calculatedX@108#calculatedY@98#children@#name@accumulatingSnapshot#id@1729452611#parent@#x@86#y@78
Summarizes the measurement events occurring at ~*predictable steps~* between the beginning and the end of a process
There is a ~*date foreign key~* in the fact table for each step in the process of the pipeline
As pipeline progress occurs, the accumulating fact table row is ~*revisited and updated~*.
This consistent updating of accumulating snapshot fact rows is unique among the three types of fact tables
#?#box#calculatedX@108#calculatedY@98#children@#name@hybridInmonKimbal#id@1720961854#parent@#x@86#y@78
hybrid CIF KIMBALL
staging is 3NF but off limits to users 
Presentation area has all Kimball qualities eg: atomic dimensions